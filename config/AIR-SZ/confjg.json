{
    "datasetParams": {
        "data_root_path": "data/AIR-SZ/",
        "index_file_path": "index_in24_out24.pkl",
        "data_file_path": "data_in24_out24.pkl",
        "feat_dims": [1,2]
    },
    "modelParams":{
        "llmair": {
            "batch_size": 32,
            "learning_rate": 5e-4,
            "weight_decay":0.0,
            "seq_len": 24,
            "pred_len": 24,
            "dropout": 0.1,
            "alpha": 0.1
        },
        "gpt_st": {
            "predict_model": "mtgnn",
            "batch_size": 32,
            "learning_rate": 5e-3,
            "weight_decay": 0,
            "change_epoch": 10,
            "input_base_dim": 10,
            "seq_len": 24,
            "pred_len":24,
            "horizon": 24,
            "mode": "eval",
            "num_nodes":35,
            "mask_ratio": 0.25,
            "ada_mask_ratio": 0.5,
            "ada_type": "half",
            "hidden_dim": 32,
            "embed_dim": 16,
            "embed_dim_spa": 4,
            "output_dim": 1,
            "HS": 10, 
            "HT": 16,
            "HT_Tem": 8,
            "num_route": 3
        },
        "timellm": {
            "batch_size": 128,
            "learning_rate": 1e-4,
            "weight_decay": 0.0,
            "pred_step":24,
            "patch_len":8,
            "stride":2,
            "enc_in":1,
            "seq_len": 24,
            "pred_len": 24,
            "label_len":12,
            "d_ff":32,
            "d_model":32,
            "n_heads":2,
            "llm_layers":6,
            "c_out":1,
            "dropout":0.0
        },
        "allm4ts": {
            "batch_size": 128,
            "learning_rate": 1e-4,
            "weight_decay": 0.0,
            "is_llm":0,
            "llm":"./HF_MODELS/gpt2",
            "freeze":0,
            "pretrain":1,
            "c_pt":0,
            "revin":1,
            "affine":0,
            "sft":0,
            "sft_layers":"null",
            "pt_layers":"null",
            "subtract_last":0,
            "pred_step":24,
            "patch_len":8,
            "stride":2,
            "enc_in":1,
            "seq_len": 24,
            "pred_len": 24,
            "label_len":12,
            "d_ff":32,
            "d_model":32,
            "n_heads":2,
            "llm_layers":1,
            "c_out":1,
            "dropout":0.0
        },
        "gpt4ts": {
            "batch_size": 128,
            "learning_rate": 1e-4,
            "weight_decay": 0.1,
            "pred_step":24,
            "patch_size":1,
            "stride":2,
            "enc_in":10,
            "seq_len": 24,
            "pred_len": 24,
            "label_len":12,
            "d_ff":32,
            "d_model":32,
            "n_heads":2,
            "mlp":0,
            "c_out":1,
            "dropout":0.0
        }
    }
  }